{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base imports\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import enlighten\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU, GRU\n",
    "import torch_geometric.transforms as T\n",
    "from torch_sparse import coalesce\n",
    "from torch_geometric.data import (InMemoryDataset, Data, download_url)\n",
    "from torch_scatter import scatter\n",
    "\n",
    "from torch_geometric.nn import NNConv, Set2Set\n",
    "# from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import remove_self_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDKit\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdchem import HybridizationType\n",
    "from rdkit.Chem.rdchem import BondType as BT\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit import RDLogger\n",
    "\n",
    "RDLogger.logger().setLevel(RDLogger.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fcts\n",
    "def show(df):\n",
    "    \"\"\"Render the molecules within a DataFrame correctly\"\"\"\n",
    "    return HTML(df.to_html(notebook=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay = 1511\n",
    "global dataset\n",
    "# filename = f'{assay}_dataset.pt'\n",
    "dataset = f'{assay}_dataset.pt'\n",
    "global input_sdf\n",
    "input_sdf = f'{assay}_compounds.sdf'\n",
    "global directory\n",
    "directory = f'{assay}_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup dataset\n",
    "sdfpath = os.path.join(os.getcwd(), f'{assay}_compounds.sdf')\n",
    "backup_df = PandasTools.LoadSDF(sdfpath)\n",
    "dataframe = backup_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(dataframe.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "global element_dict\n",
    "elements = []\n",
    "\n",
    "if os.path.exists('element_list.json'):\n",
    "    pass\n",
    "else:\n",
    "    with open('periodictable.csv', 'r') as ifile:\n",
    "        reader = csv.reader(ifile)\n",
    "        for i in reader:\n",
    "            element = i[1]\n",
    "            elements.append(element)\n",
    "\n",
    "    indexes = [i for i in range(len(elements))]\n",
    "\n",
    "\n",
    "    element_dict = dict(zip(elements, indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "if os.path.exists('element_list.json'):\n",
    "    with open('element_list.json', 'r') as listfile:\n",
    "        element_dict = json.load(listfile)\n",
    "else:\n",
    "    with open('element_list.json', 'w') as listfile:\n",
    "        json.dump(element_dict, listfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup filesystem\n",
    "import shutil\n",
    "\n",
    "rawdir = os.path.join(directory, 'raw')\n",
    "processdir = os.path.join(directory, 'processed')\n",
    "\n",
    "try:\n",
    "    os.mkdir(directory)\n",
    "    # rawdir = os.path.join(directory, 'raw')\n",
    "    # processdir = os.path.join(directory, 'processed')\n",
    "    os.mkdir(rawdir)\n",
    "    os.mkdir(processdir)\n",
    "except FileExistsError:\n",
    "    print(\"Directory already exists\")\n",
    "    pass\n",
    "finally:\n",
    "    shutil.copyfile(os.path.abspath(input_sdf), os.path.join(rawdir, input_sdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"PUBCHEM_ACTIVITY_OUTCOME\"].replace(\"Inactive\", 0, inplace=True)\n",
    "dataframe[\"PUBCHEM_ACTIVITY_OUTCOME\"].replace(\"Active\", 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.PUBCHEM_ACTIVITY_OUTCOME.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global targets\n",
    "targets = dataframe.PUBCHEM_ACTIVITY_OUTCOME.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioAssayDataset(InMemoryDataset):\n",
    "    def __init__(self, root=None, transform=None, pre_transform=None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [input_sdf]\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [dataset]\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "\n",
    "        bonds = {BT.SINGLE: 0, BT.DOUBLE: 1, BT.TRIPLE: 2, BT.AROMATIC: 3}\n",
    "        types = element_dict\n",
    "\n",
    "\n",
    "        # mol_list = Chem.SDMolSupplier(self.raw_paths[0], removeHs=False, sanitize=False)\n",
    "        mol_list = Chem.SDMolSupplier(self.raw_paths[0], removeHs=False)\n",
    "        pbar = enlighten.Counter(total=len(mol_list), desc='Encoding molecules...', unit='ticks')\n",
    "        for ID, mol in enumerate(mol_list):\n",
    "            #number of atoms\n",
    "            n_a = mol.GetNumAtoms()\n",
    "\n",
    "\n",
    "            conf = mol.GetConformer()\n",
    "            pos = conf.GetPositions()\n",
    "            pos = torch.tensor(pos, dtype=torch.float)\n",
    "\n",
    "            #atom features\n",
    "            type_idx = []\n",
    "            atomic_number = []\n",
    "            aromatic = []\n",
    "            sp = []\n",
    "            sp2 = []\n",
    "            sp3 = []\n",
    "            num_hs = []\n",
    "        \n",
    "            for atom in mol.GetAtoms():\n",
    "                #need to implement this\n",
    "                type_idx.append(types[atom.GetSymbol()])\n",
    "\n",
    "                atomic_number.append(atom.GetAtomicNum())\n",
    "                aromatic.append(1 if atom.GetIsAromatic() else 0)\n",
    "                hybridisation = atom.GetHybridization()\n",
    "                sp.append(1 if hybridisation == HybridizationType.SP else 0)\n",
    "                sp2.append(1 if hybridisation == HybridizationType.SP2 else 0)\n",
    "                sp3.append(1 if hybridisation == HybridizationType.SP3 else 0)\n",
    "                num_hs.append(atom.GetTotalNumHs(includeNeighbors=True))\n",
    "\n",
    "            z = torch.tensor(atomic_number, dtype=torch.long)            \n",
    "\n",
    "            #bonds\n",
    "            row, col, edge_type = [], [], []\n",
    "            for bond in mol.GetBonds():\n",
    "                start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "                row += [start, end]\n",
    "                col += [end, start]\n",
    "                edge_type += 2 * [bonds[bond.GetBondType()]]\n",
    "\n",
    "            edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "            edge_type = torch.tensor(edge_type, dtype=torch.long)\n",
    "            edge_attr = F.one_hot(edge_type,num_classes=len(bonds)).to(torch.float)\n",
    "\n",
    "            # perm = (edge_index[0] * n_a + edge_index[1]).argsort()\n",
    "\n",
    "            # # print(edge_index[0])\n",
    "            # # print(edge_index[0])\n",
    "            # # print(n_a)\n",
    "            # edge_index = edge_index[:, perm]\n",
    "            # edge_type = edge_type[perm]\n",
    "            # edge_attr = edge_attr[perm]\n",
    "\n",
    "            # row, col = edge_index\n",
    "            # hs = (z==1).to(torch.float)\n",
    "            # num_hs = scatter(hs[row], col, dim_size=n_a).tolist()\n",
    "\n",
    "            x1 = F.one_hot(torch.tensor(type_idx), num_classes=len(types))\n",
    "            x2 = torch.tensor([atomic_number, aromatic, sp, sp2, sp3, num_hs], dtype=torch.float).t().contiguous()\n",
    "            x = torch.cat([x1.to(torch.float), x2], dim=-1)\n",
    "\n",
    "            #target is the binarised activities\n",
    "            y = targets[ID]\n",
    "\n",
    "            name = mol.GetProp('_Name')\n",
    "\n",
    "            data = Data(x=x, z=z, pos=pos, edge_index=edge_index, edge_attr=edge_attr, y=y, name=name, idx=ID)\n",
    "            data_list.append(data)\n",
    "            pbar.update()\n",
    "        torch.save(self.collate(data_list), self.processed_paths[0])\n",
    "\n",
    "    #need to add process functions for the molecules\n",
    "\n",
    "# class MyTransfrom(object):\n",
    "#     def __call__(self, data):\n",
    "#         print(data)\n",
    "#         data.y = data.y[:, targets]\n",
    "#         print(data.y)\n",
    "#         return data\n",
    "        \n",
    "class Complete(object):\n",
    "    def __call__(self, data):\n",
    "        device = data.edge_index.device\n",
    "\n",
    "        row = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
    "        col = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
    "\n",
    "        row = row.view(-1, 1).repeat(1, data.num_nodes).view(-1)\n",
    "        col = col.repeat(data.num_nodes)\n",
    "        edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "        edge_attr = None\n",
    "        if data.edge_attr is not None:\n",
    "            idx = data.edge_index[0] * data.num_nodes + data.edge_index[1]\n",
    "            size = list(data.edge_attr.size())\n",
    "            size[0] = data.num_nodes * data.num_nodes\n",
    "            edge_attr = data.edge_attr.new_zeros(size)\n",
    "            edge_attr[idx] = data.edge_attr\n",
    "\n",
    "        edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
    "        data.edge_attr = edge_attr\n",
    "        data.edge_index = edge_index\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([Complete(), T.Distance(norm=False)])\n",
    "\n",
    "dataset = BioAssayDataset(directory, transform=transform).shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data.y.view(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dataset:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split datasets into train/test\n",
    "\n",
    "td_len = len(dataset)\n",
    "tr_part = round(td_len*0.8)\n",
    "val_part = round(td_len*0.1)\n",
    "\n",
    "test_set = dataset[(tr_part+val_part):]\n",
    "val_set = dataset[tr_part:(tr_part+val_part)]\n",
    "train_set = dataset[:tr_part]\n",
    "\n",
    "train_loader = DataLoader(test_set, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=2160, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup torch NN\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.lin0 = torch.nn.Linear(dataset.num_features, dim)\n",
    "\n",
    "        nn = Sequential(Linear(5,128), ReLU(), Linear(128, dim * dim))\n",
    "        self.conv = NNConv(dim, dim, nn, aggr='mean')\n",
    "        self.gru = GRU(dim, dim)\n",
    "\n",
    "        self.set2set = Set2Set(dim, processing_steps=3)\n",
    "        self.lin1 = torch.nn.Linear(2 * dim, dim)\n",
    "        self.lin2 = torch.nn.Linear(dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        out = F.relu(self.lin0(data.x))\n",
    "        h = out.unsqueeze(0)\n",
    "\n",
    "        for i in range(3):\n",
    "            m = F.relu(self.conv(out, data.edge_index, data.edge_attr))\n",
    "            out, h = self.gru(m.unsqueeze(0), h)\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "        out = self.set2set(out, data.batch)\n",
    "        out = F.relu(self.lin1(out))\n",
    "        out = self.lin2(out)\n",
    "\n",
    "        return out.view(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min',\n",
    "                                                        factor=0.7, patience=5,\n",
    "                                                        min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        loss = F.mse_loss(model(data), data.y)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        optimiser.step()\n",
    "\n",
    "    return loss_all / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    error = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        error += (model(data) - data.y).abs().sum().item()\n",
    "        return error / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_error = None\n",
    "\n",
    "for epoch in range(1, max_epochs):\n",
    "    lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    loss = train(epoch)\n",
    "    val_error = test(val_loader)\n",
    "    scheduler.step(val_error)\n",
    "\n",
    "    if best_val_error is None or val_error <= best_val_error:\n",
    "        test_error = test(test_loader)\n",
    "        best_val_error = val_error\n",
    "\n",
    "    print(f' Epoch {epoch}, LR: {lr}, Loss: {loss}, Validation MAE: {val_error}, Test MAE: {test_error}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbcbd352f3a1bc1f772970a8ec0a6a21791151c1068f88a3a88dc0a1196b67fe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('assay_geo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
